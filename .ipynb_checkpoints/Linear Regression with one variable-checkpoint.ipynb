{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zf3lciEqn7-Y"
   },
   "source": [
    "# Linear Regression with one variable\n",
    "\n",
    "Welcome to your first lab! You will build a linear regression with one variable.\n",
    "\n",
    "Your first task will be prediction of kangaroo nasal width having its length.\n",
    "\n",
    "**Instructions:**\n",
    "- Do not use loops (for/while) in your code, unless the instructions explicitly ask you to do so.\n",
    "\n",
    "**You will learn to:**\n",
    "- Build the general architecture of a learning algorithm, including:\n",
    "    - Initializing parameters\n",
    "    - Calculating the cost function and its gradient\n",
    "    - Using an optimization algorithm (gradient descent) \n",
    "- Gather all three functions above into a main model function, in the right order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CYAHd0Gen7-a"
   },
   "source": [
    "## 1 - Packages ##\n",
    "\n",
    "First, let's run the cell below to import all the packages that you will need during this assignment. \n",
    "- [numpy](www.numpy.org) is the fundamental package for scientific computing with Python.\n",
    "- [matplotlib](http://matplotlib.org) is a famous library to plot graphs in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "BIosQV9jn7-a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1cQ3hKSn7-e"
   },
   "source": [
    "## 2 - Overview of the Problem set ##\n",
    "\n",
    "**Problem Statement**: You are given a dataset  containing:\n",
    "    - a training set of m_train examples\n",
    "    - a test set of m_test examples\n",
    "\n",
    "Let's get more familiar with the dataset. Load the data by running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OMj2MqS1n7-e"
   },
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "\n",
    "def load_data():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    data = np.genfromtxt('kangaroo.csv', delimiter=',')\n",
    "    \n",
    "    x = data[:, 0]\n",
    "    y = data[:, 1]\n",
    "    \n",
    "    train_set_x, test_set_x, train_set_y, test_set_y = train_test_split(x, y, test_size=0.33, random_state=42)\n",
    "        \n",
    "    return train_set_x, test_set_x, train_set_y, test_set_y\n",
    "\n",
    "train_set_x, test_set_x, train_set_y, test_set_y = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xm-osSmTn7-g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30,) (15,) (30,) (15,)\n"
     ]
    }
   ],
   "source": [
    "print (train_set_x.shape, test_set_x.shape, train_set_y.shape, test_set_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z9FpR2-zn7-j"
   },
   "source": [
    "Many software bugs in machine learning come from having matrix/vector dimensions that don't fit. If you can keep your matrix/vector dimensions straight you will go a long way toward eliminating many bugs. \n",
    "\n",
    "**Exercise:** Find the values for:\n",
    "    - m_train (number of training examples)\n",
    "    - m_test (number of test examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "6QyiYK7Mn7-k"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-1ff7cad9a7a1>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-1ff7cad9a7a1>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    m_train =\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "### START CODE HERE ### (≈ 2 lines of code)\n",
    "m_train = \n",
    "m_test = \n",
    "### END CODE HERE ###\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"train_set_x shape: \" + str(train_set_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x shape: \" + str(test_set_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_set_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rGgoXnawn7-l"
   },
   "source": [
    "**Expected Output for m_train and m_test**: \n",
    "<table style=\"width:15%\">\n",
    "  <tr>\n",
    "    <td><b>m_train</b></td>\n",
    "    <td> 30 </td> \n",
    "  </tr>\n",
    "  \n",
    "  <tr>\n",
    "    <td><b>m_test</b></td>\n",
    "    <td> 15 </td> \n",
    "  </tr>\n",
    "  \n",
    "\n",
    "  \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irg0zNM0n7-m"
   },
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8ehMZXUNn7-m"
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(train_set_x, train_set_y)\n",
    "plt.title(\"Training set\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Width\");\n",
    "\n",
    "# Test set\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(test_set_x, test_set_y)\n",
    "plt.title(\"Test set\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Width\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LhBfKBk4n7-o"
   },
   "source": [
    "### Standardization\n",
    "One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array.\n",
    "\n",
    "$$X_{new}= \\frac{X - \\mu}{\\sigma}$$\n",
    "\n",
    "Let's standardize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "HDhE37Aan7-p"
   },
   "outputs": [],
   "source": [
    "mean = np.concatenate([train_set_x,test_set_x]).mean()\n",
    "std = np.concatenate([train_set_x,test_set_x]).std()\n",
    "\n",
    "train_set_x = (train_set_x - mean) / std\n",
    "test_set_x = (test_set_x - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bba7Qb52n7-r"
   },
   "source": [
    "### Standardized data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "XRjvqMhjn7-r"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(train_set_x, train_set_y)\n",
    "plt.title(\"Training set\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Width\");\n",
    "\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.scatter(test_set_x, test_set_y)\n",
    "plt.title(\"Test set\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Width\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4vrMF9_3n7-w"
   },
   "source": [
    "## 3 - General Architecture of the learning algorithm ##\n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "\n",
    "$$h^{(i)} = \\theta x^{(i)} + b\\tag{1}$$\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$J(\\theta, b) = \\frac{1}{2m}\\sum_{i=1}^{m}(h^{(i)}- y^{(i)})^{2}\\tag{2}$$ \n",
    "\n",
    "**Key steps**:\n",
    "In this exercise, you will carry out the following steps: \n",
    "    - Initialize the parameters of the model\n",
    "    - Learn the parameters for the model by minimizing the cost  \n",
    "    - Use the learned parameters to make predictions (on the test set)\n",
    "    - Analyse the results and conclude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sTai6oVtn7-w"
   },
   "source": [
    "## 4 - Building the parts of our algorithm ## \n",
    "\n",
    "The main steps for building a learning algoritm:\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "You often build 1-3 separately and integrate them into one function we call `model()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w5qOAQain7-x"
   },
   "source": [
    "### 4.1 - Initializing parameters\n",
    "\n",
    "**Exercise:** Implement parameter initialization in the cell below. You have to initialize \n",
    "$\\theta$ as zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "7yEbtj7on7-x"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_with_zeros\n",
    "\n",
    "def initialize_with_zeros():\n",
    "    \"\"\"\n",
    "    This function initializes parameters theta and b as 0.\n",
    "    \n",
    "    Returns:\n",
    "    theta -- initialized scalar parameter\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 2 line of code)\n",
    "    theta = \n",
    "    b = \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    assert(isinstance(theta, int))\n",
    "    assert(isinstance(b, int))\n",
    "    \n",
    "    return theta, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Ld4GufDfn7-z"
   },
   "outputs": [],
   "source": [
    "theta, b = initialize_with_zeros()\n",
    "print (\"theta = \" + str(theta))\n",
    "print (\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Djb2S4Kwn7-1"
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "\n",
    "<table style=\"width:15%\">\n",
    "    <tr>\n",
    "        <td><b>theta</b></td>\n",
    "        <td> 0\n",
    "  </td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "            <td><b>b</b></td>\n",
    "            <td> 0 </td>\n",
    "        </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GGCNi1s0n7-1"
   },
   "source": [
    "### 4.2 - Forward and Backward propagation\n",
    "\n",
    "Now that your parameters are initialized, you can do the \"forward\" and \"backward\" propagation steps for learning the parameters.\n",
    "\n",
    "**Exercise:** Implement a function `propagate()` that computes the cost function and its gradient.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "Forward Propagation:\n",
    "- You get X\n",
    "- You compute $H = (\\theta X + b) = (h^{(1)}, h^{(2)}, ..., h^{(m-1)}, h^{(m)})$\n",
    "- You calculate the cost function: $J(\\theta, b) = \\frac{1}{2m}\\sum_{i=1}^{m}(h^{(i)} - y^{(i)})^{2}$\n",
    "\n",
    "\n",
    "Here is the formula of gradient of the cost function: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\frac{1}{m}X(H-Y)^T\\tag{3}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (h^{(i)}-y^{(i)})\\tag{4}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fv3K9lt7n7-2"
   },
   "outputs": [],
   "source": [
    "def propagate(theta, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    theta -- parameter, a scalar\n",
    "    b -- bias, a scalar\n",
    "    X -- features vector of size (number of examples, )\n",
    "    Y -- results vector (number of examples, )\n",
    "\n",
    "    Return:\n",
    "    cost -- cost function for linear regression\n",
    "    dt -- gradient of the loss with respect to theta, thus same shape as theta\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation.\n",
    "    - Use np.dot() to avoid for-loops in favor of code vectorization\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # FORWARD PROPAGATION (FROM X TO COST)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    H =         # compute activation\n",
    "    cost =      # compute cost\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    dt = \n",
    "    db = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(dt.dtype == float)\n",
    "    assert(db.dtype == float)\n",
    "    cost = np.squeeze(cost)\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    grads = {\"dt\": dt,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "K5Zudl1yn7-5"
   },
   "outputs": [],
   "source": [
    "theta, b, X, Y = 1., 2., np.array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6]), np.array([2, 3, 4, 5, 6, 7])\n",
    "grads, cost = propagate(theta, b, X, Y)\n",
    "print (\"dt = \" + str(grads[\"dt\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cb6q9bEHn7-8"
   },
   "source": [
    "**Expected Output**:\n",
    "\n",
    "<table style=\"width:20%\">\n",
    "    <tr>\n",
    "        <td><b>dt</b></td>\n",
    "      <td> -1.015 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>db</b></td>\n",
    "        <td> -2.15 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>cost</b></td>\n",
    "        <td> 3.4925 </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YGIqE0Jpn7-9"
   },
   "source": [
    "### 4.3 - Optimization\n",
    "- You have initialized your parameters.\n",
    "- You are also able to compute a cost function and its gradient.\n",
    "- Now, you want to update the parameters using gradient descent.\n",
    "\n",
    "**Exercise:** Write down the optimization function. The goal is to learn $\\theta$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } \\partial{}J$, where $\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YR_M-wiXn7--"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: optimize\n",
    "\n",
    "def optimize(theta, b, X, Y, num_iterations, learning_rate, print_cost=False):\n",
    "    \"\"\"\n",
    "    This function optimizes theta and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    theta -- parameter, a scalar\n",
    "    b -- bias, a scalar\n",
    "    X -- features vector of shape (number of examples, )\n",
    "    Y -- results vector of shape (number of examples, )\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights theta and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for theta and b.\n",
    "    \"\"\"\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        \n",
    "        # Cost and gradient calculation (≈ 1 line of code)\n",
    "        ### START CODE HERE ### \n",
    "        grads, cost = \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve derivatives from grads\n",
    "        dt = grads[\"dt\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule (≈ 2 lines of code)\n",
    "        ### START CODE HERE ###\n",
    "        theta = \n",
    "        b = \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"theta\": theta,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dt\": dt,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "-vcWkBHKn7_A"
   },
   "outputs": [],
   "source": [
    "params, grads, costs = optimize(theta, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False)\n",
    "print (\"theta = \" + str(params[\"theta\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dt = \" + str(grads[\"dt\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlEL9EcMn7_C"
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:30%\">\n",
    "    <tr>\n",
    "       <td style=\"width:10%\"><b>theta</b></td>\n",
    "       <td>  1.6451645666550938 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td><b>b</b></td>\n",
    "       <td> 3.1951480476314393 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td><b>dt</b></td>\n",
    "       <td> -0.5018556276988992 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "       <td><b>db</b></td>\n",
    "       <td> -0.7372605441640054 </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q0MXWhvpn7_C"
   },
   "source": [
    "**Exercise:** The previous function will output the learned $\\theta$ and $b$. We are able to use $\\theta$ and $b$ to predict the labels for a dataset X. Implement the `predict()` function. It must calculate $H = \\theta X + b$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "yXUKoHwhn7_D"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(theta, b, X):\n",
    "    \"\"\"\n",
    "    Predict using learned linear regression parameters (theta, b)\n",
    "    \n",
    "    Arguments:\n",
    "    theta -- parameter, a scalar\n",
    "    b -- bias, a scalar\n",
    "    X -- features vector of size (number of examples, )\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions for the examples in X\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute vector \"Y_prediction\" predicting the width of a kangoroo nasal\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Y_prediction = \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DL05357Qn7_E"
   },
   "outputs": [],
   "source": [
    "theta = 0.1124579\n",
    "b = -0.3\n",
    "X = np.array([1., -1.1, -3.2, 1.2, 2., 0.1])\n",
    "print (\"predictions = \" + str(predict(theta, b, X)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G_zp8Fz1n7_G"
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:30%\">\n",
    "    <tr>\n",
    "         <td>\n",
    "             <b>predictions</b>\n",
    "         </td>\n",
    "          <td>\n",
    "            [-0.1875421  -0.42370369 -0.65986528 -0.16505052 -0.0750842  -0.28875421]\n",
    "         </td>  \n",
    "   </tr>\n",
    "\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k1AhH_L2n7_G"
   },
   "source": [
    "<font color='green'>\n",
    "    <b>What to remember:</b>\n",
    "You've implemented several functions that:\n",
    "- Initialize (theta,b)\n",
    "- Optimize the loss iteratively to learn parameters (theta,b):\n",
    "    - computing the cost and its gradient \n",
    "    - updating the parameters using gradient descent\n",
    "- Use the learned (theta,b) to predict the value for a given set of examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lLeEhD1_n7_H"
   },
   "source": [
    "## 5 - Merge all functions into a model ##\n",
    "\n",
    "You will now see how the overall model is structured by putting together all the building blocks (functions implemented in the previous parts) together, in the right order.\n",
    "\n",
    "**Exercise:** \n",
    "\n",
    "Implement the model function. Use the following notation:\n",
    "    - Y_prediction_test for your predictions on the test set\n",
    "    - Y_prediction_train for your predictions on the train set\n",
    "    - theta, costs, grads for the outputs of optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UU1SpM-zn7_I"
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: model\n",
    "\n",
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the linear regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Arguments:\n",
    "    X_train -- training set represented by a numpy array of shape (m_train, )\n",
    "    Y_train -- training values represented by a numpy array (vector) of shape (m_train, )\n",
    "    X_test -- test set represented by a numpy array of shape (m_test, )\n",
    "    Y_test -- test values represented by a numpy array (vector) of shape (m_test, )\n",
    "    num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "    print_cost -- Set to true to print the cost every 100 iterations\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # initialize parameters with zeros (≈ 1 line of code)\n",
    "    theta, b = \n",
    "\n",
    "    # Gradient descent (≈ 1 line of code)\n",
    "    parameters, grads, costs = \n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    theta = parameters[\"theta\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples (≈ 2 lines of code)\n",
    "    Y_prediction_test = \n",
    "    Y_prediction_train = \n",
    "\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print (\"Train RMSE: {} \".format(np.sqrt(np.mean((Y_prediction_train - Y_train) ** 2))))\n",
    "    print (\"Test RMSE: {} \".format(np.sqrt(np.mean((Y_prediction_test - Y_test) ** 2))))\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"theta\" : theta, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fD31EzjLn7_J"
   },
   "outputs": [],
   "source": [
    "d = model(train_set_x, train_set_y, test_set_x, test_set_y, num_iterations=500, learning_rate=0.05, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fl84LOXRn7_L"
   },
   "source": [
    "**Expected Output**: \n",
    "\n",
    "<table style=\"width:45%\"> \n",
    "    <tr>\n",
    "        <td><b>Cost after iteration 0</b>  </td> \n",
    "        <td> 30753.266667 </td>\n",
    "    </tr>\n",
    "      <tr>\n",
    "        <td> <center> $\\vdots$ </center> </td> \n",
    "        <td> <center> $\\vdots$ </center> </td> \n",
    "    </tr>  \n",
    "    <tr>\n",
    "        <td><b>Train RMSE</b></td> \n",
    "        <td> 13.25047088422024 </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><b>Test RMSE</b></td> \n",
    "        <td> 16.20421532059726 </td>\n",
    "    </tr>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s0wQzIPIn7_L"
   },
   "source": [
    "### Les't check the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "jN19yAxrn7_N"
   },
   "outputs": [],
   "source": [
    "# Training set\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.title(\"Training set\")\n",
    "\n",
    "plt.scatter(train_set_x, train_set_y)\n",
    "x = np.array([min(train_set_x), max(train_set_x)])\n",
    "theta = d[\"theta\"]\n",
    "b = d[\"b\"]\n",
    "y = theta * x + b\n",
    "plt.plot(x, y)  \n",
    "plt.axis(\"tight\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Width\");\n",
    "plt.tight_layout()\n",
    "\n",
    "\n",
    "# Test set\n",
    "plt.figure(figsize=(4, 3))\n",
    "plt.title(\"Test set\")\n",
    "\n",
    "plt.scatter(test_set_x, test_set_y)\n",
    "x = np.array([min(test_set_x), max(test_set_x)])\n",
    "theta = d[\"theta\"]\n",
    "b = d[\"b\"]\n",
    "y = theta * x + b\n",
    "plt.plot(x, y)  \n",
    "plt.axis(\"tight\")\n",
    "plt.xlabel(\"Length\")\n",
    "plt.ylabel(\"Width\");\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "Linear Regression with one variable.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
